// SPDX-License-Identifier: BUSL-1.1
// Copyright (c) 2026 Alfred Jean LLC

//! Mutation handlers for state-changing requests.

use std::sync::Arc;

use parking_lot::Mutex;

use oj_core::{AgentId, Event, PipelineId, SessionId, ShortId, WorkspaceId};
use oj_engine::breadcrumb::Breadcrumb;
use oj_runbook::Runbook;
use oj_storage::MaterializedState;

use crate::event_bus::EventBus;
use crate::protocol::{
    AgentEntry, CronEntry, PipelineEntry, Response, WorkerEntry, WorkspaceEntry,
};

use super::ConnectionError;

/// Handle a status request.
pub(super) fn handle_status(
    state: &Arc<Mutex<MaterializedState>>,
    orphans: &Arc<Mutex<Vec<Breadcrumb>>>,
    start_time: std::time::Instant,
) -> Response {
    let uptime_secs = start_time.elapsed().as_secs();
    let (pipelines_active, sessions_active) = {
        let state = state.lock();
        let active = state
            .pipelines
            .values()
            .filter(|p| !p.is_terminal())
            .count();
        let sessions = state.sessions.len();
        (active, sessions)
    };
    let orphan_count = orphans.lock().len();

    Response::Status {
        uptime_secs,
        pipelines_active,
        sessions_active,
        orphan_count,
    }
}

/// Handle a session send request.
pub(super) fn handle_session_send(
    state: &Arc<Mutex<MaterializedState>>,
    event_bus: &EventBus,
    id: String,
    input: String,
) -> Result<Response, ConnectionError> {
    let session_id = {
        let state_guard = state.lock();
        if state_guard.sessions.contains_key(&id) {
            Some(id.clone())
        } else {
            state_guard
                .pipelines
                .get(&id)
                .and_then(|p| p.session_id.clone())
        }
    };

    match session_id {
        Some(sid) => {
            let event = Event::SessionInput {
                id: SessionId::new(sid),
                input,
            };
            event_bus
                .send(event)
                .map_err(|_| ConnectionError::WalError)?;
            Ok(Response::Ok)
        }
        None => Ok(Response::Error {
            message: format!("Session not found: {}", id),
        }),
    }
}

/// Handle a session kill request.
///
/// Validates that the session exists, kills the tmux session, and emits
/// a SessionDeleted event to clean up state.
pub(super) async fn handle_session_kill(
    state: &Arc<Mutex<MaterializedState>>,
    event_bus: &EventBus,
    id: &str,
) -> Result<Response, ConnectionError> {
    let session_id = {
        let state_guard = state.lock();
        if state_guard.sessions.contains_key(id) {
            Some(id.to_string())
        } else {
            None
        }
    };

    match session_id {
        Some(sid) => {
            // Kill the tmux session
            let _ = tokio::process::Command::new("tmux")
                .args(["kill-session", "-t", &sid])
                .output()
                .await;

            // Emit SessionDeleted to clean up state
            let event = Event::SessionDeleted {
                id: SessionId::new(sid),
            };
            event_bus
                .send(event)
                .map_err(|_| ConnectionError::WalError)?;
            Ok(Response::Ok)
        }
        None => Ok(Response::Error {
            message: format!("Session not found: {}", id),
        }),
    }
}

/// Handle a pipeline resume request.
///
/// Validates that the pipeline exists in state or the orphan registry before
/// emitting the resume event. For orphaned pipelines, emits synthetic events
/// to reconstruct the pipeline in state, then resumes.
pub(super) fn handle_pipeline_resume(
    state: &Arc<Mutex<MaterializedState>>,
    orphans: &Arc<Mutex<Vec<Breadcrumb>>>,
    event_bus: &EventBus,
    id: String,
    message: Option<String>,
    vars: std::collections::HashMap<String, String>,
) -> Result<Response, ConnectionError> {
    // Check if pipeline exists in state and get relevant info for validation
    let pipeline_info = {
        let state_guard = state.lock();
        state_guard.get_pipeline(&id).map(|p| {
            (
                p.id.clone(),
                p.kind.clone(),
                p.step.clone(),
                p.runbook_hash.clone(),
            )
        })
    };

    if let Some((pipeline_id, pipeline_kind, current_step, runbook_hash)) = pipeline_info {
        // Validate agent steps require --message before emitting event
        if message.is_none() && current_step != "failed" {
            if let Err(err_msg) = validate_resume_message(
                state,
                &pipeline_id,
                &pipeline_kind,
                &current_step,
                &runbook_hash,
            ) {
                return Ok(Response::Error { message: err_msg });
            }
        }

        let event = Event::PipelineResume {
            id: PipelineId::new(pipeline_id),
            message,
            vars,
        };
        event_bus
            .send(event)
            .map_err(|_| ConnectionError::WalError)?;
        return Ok(Response::Ok);
    }

    // Not in state — check orphan registry
    let orphan = {
        let orphans_guard = orphans.lock();
        orphans_guard
            .iter()
            .find(|bc| bc.pipeline_id == id || bc.pipeline_id.starts_with(&id))
            .cloned()
    };

    let Some(orphan) = orphan else {
        return Ok(Response::Error {
            message: format!("pipeline not found: {}", id),
        });
    };

    // Orphan found — check if the runbook is available for reconstruction
    if orphan.runbook_hash.is_empty() {
        return Ok(Response::Error {
            message: format!(
                "pipeline {} is orphaned (state lost during daemon restart) and cannot be \
                 resumed: breadcrumb missing runbook hash (written by older daemon version). \
                 Dismiss with `oj pipeline prune --orphans` and re-run the pipeline.",
                orphan.pipeline_id
            ),
        });
    }

    // Verify the runbook is in state (needed for step definitions during resume)
    let runbook_available = {
        let state_guard = state.lock();
        state_guard.runbooks.contains_key(&orphan.runbook_hash)
    };

    if !runbook_available {
        return Ok(Response::Error {
            message: format!(
                "pipeline {} is orphaned (state lost during daemon restart) and cannot be \
                 resumed: runbook is no longer available. Dismiss with \
                 `oj pipeline prune --orphans` and re-run the pipeline.",
                orphan.pipeline_id
            ),
        });
    }

    // Reconstruct the pipeline by emitting synthetic events:
    // 1. PipelineCreated (at current_step as initial_step)
    // 2. PipelineAdvanced to "failed" (so resume resets to the right step)
    // 3. PipelineResume (the actual resume request)
    let orphan_id = orphan.pipeline_id.clone();
    let pipeline_id = PipelineId::new(&orphan_id);
    let cwd = orphan.cwd.or(orphan.workspace_root).unwrap_or_default();

    event_bus
        .send(Event::PipelineCreated {
            id: pipeline_id.clone(),
            kind: orphan.kind,
            name: orphan.name,
            runbook_hash: orphan.runbook_hash,
            cwd,
            vars: orphan.vars,
            initial_step: orphan.current_step,
            created_at_epoch_ms: 0,
            namespace: orphan.project,
            cron_name: None,
        })
        .map_err(|_| ConnectionError::WalError)?;

    event_bus
        .send(Event::PipelineAdvanced {
            id: pipeline_id.clone(),
            step: "failed".to_string(),
        })
        .map_err(|_| ConnectionError::WalError)?;

    event_bus
        .send(Event::PipelineResume {
            id: pipeline_id,
            message,
            vars,
        })
        .map_err(|_| ConnectionError::WalError)?;

    // Remove from orphan registry
    {
        let mut orphans_guard = orphans.lock();
        if let Some(idx) = orphans_guard
            .iter()
            .position(|bc| bc.pipeline_id == orphan_id)
        {
            orphans_guard.remove(idx);
        }
    }

    Ok(Response::Ok)
}

/// Handle a pipeline cancel request.
pub(super) fn handle_pipeline_cancel(
    state: &Arc<Mutex<MaterializedState>>,
    event_bus: &EventBus,
    ids: Vec<String>,
) -> Result<Response, ConnectionError> {
    let mut cancelled = Vec::new();
    let mut already_terminal = Vec::new();
    let mut not_found = Vec::new();

    for id in ids {
        let is_valid = {
            let state_guard = state.lock();
            state_guard.get_pipeline(&id).map(|p| !p.is_terminal())
        };

        match is_valid {
            Some(true) => {
                let event = Event::PipelineCancel {
                    id: PipelineId::new(id.clone()),
                };
                event_bus
                    .send(event)
                    .map_err(|_| ConnectionError::WalError)?;
                cancelled.push(id);
            }
            Some(false) => {
                already_terminal.push(id);
            }
            None => {
                not_found.push(id);
            }
        }
    }

    Ok(Response::PipelinesCancelled {
        cancelled,
        already_terminal,
        not_found,
    })
}

/// Handle workspace drop requests.
pub(super) async fn handle_workspace_drop(
    state: &Arc<Mutex<MaterializedState>>,
    event_bus: &EventBus,
    id: Option<&str>,
    failed_only: bool,
    drop_all: bool,
) -> Result<Response, ConnectionError> {
    let workspaces_to_drop: Vec<(String, std::path::PathBuf, Option<String>)> = {
        let state = state.lock();

        if let Some(id) = id {
            // Find workspace by exact match or prefix
            let matches: Vec<_> = state
                .workspaces
                .iter()
                .filter(|(k, _)| *k == id || k.starts_with(id))
                .collect();

            if matches.len() == 1 {
                vec![(
                    matches[0].0.clone(),
                    matches[0].1.path.clone(),
                    matches[0].1.branch.clone(),
                )]
            } else if matches.is_empty() {
                return Ok(Response::Error {
                    message: format!("workspace not found: {}", id),
                });
            } else {
                return Ok(Response::Error {
                    message: format!("ambiguous workspace ID '{}': {} matches", id, matches.len()),
                });
            }
        } else if failed_only {
            state
                .workspaces
                .iter()
                .filter(|(_, w)| matches!(w.status, oj_core::WorkspaceStatus::Failed { .. }))
                .map(|(id, w)| (id.clone(), w.path.clone(), w.branch.clone()))
                .collect()
        } else if drop_all {
            state
                .workspaces
                .iter()
                .map(|(id, w)| (id.clone(), w.path.clone(), w.branch.clone()))
                .collect()
        } else {
            return Ok(Response::Error {
                message: "specify a workspace ID, --failed, or --all".to_string(),
            });
        }
    };

    let dropped: Vec<WorkspaceEntry> = workspaces_to_drop
        .iter()
        .map(|(id, path, branch)| WorkspaceEntry {
            id: id.clone(),
            path: path.clone(),
            branch: branch.clone(),
        })
        .collect();

    // Emit delete events for each workspace
    for (id, _path, _branch) in workspaces_to_drop {
        let event = Event::WorkspaceDrop {
            id: WorkspaceId::new(id),
        };
        event_bus
            .send(event)
            .map_err(|_| ConnectionError::WalError)?;
    }

    Ok(Response::WorkspacesDropped { dropped })
}

/// Handle an agent send request.
///
/// Resolves agent_id via (in order, first match wins):
/// 1. Exact agent_id match across ALL step_history entries (prefer latest)
/// 2. Pipeline ID lookup → latest agent from ALL step_history entries
/// 3. Prefix match on agent_id across ALL step_history entries (prefer latest)
/// 4. Standalone agent_runs match
/// 5. Session liveness check (tmux has-session) before returning 'not found'
pub(super) async fn handle_agent_send(
    state: &Arc<Mutex<MaterializedState>>,
    event_bus: &EventBus,
    agent_id: String,
    message: String,
) -> Result<Response, ConnectionError> {
    let resolved_agent_id = {
        let state_guard = state.lock();

        // (1) Exact agent_id match across ALL step history, prefer latest
        let mut found: Option<String> = None;
        for pipeline in state_guard.pipelines.values() {
            for record in pipeline.step_history.iter().rev() {
                if let Some(aid) = &record.agent_id {
                    if aid == &agent_id {
                        found = Some(aid.clone());
                        break;
                    }
                }
            }
            if found.is_some() {
                break;
            }
        }

        // (2) Pipeline ID lookup → latest agent from ALL step history
        if found.is_none() {
            if let Some(pipeline) = state_guard.get_pipeline(&agent_id) {
                for record in pipeline.step_history.iter().rev() {
                    if let Some(aid) = &record.agent_id {
                        found = Some(aid.clone());
                        break;
                    }
                }
            }
        }

        // (3) Prefix match across ALL step history entries, prefer latest
        if found.is_none() {
            for pipeline in state_guard.pipelines.values() {
                for record in pipeline.step_history.iter().rev() {
                    if let Some(aid) = &record.agent_id {
                        if aid.starts_with(&agent_id) {
                            found = Some(aid.clone());
                            break;
                        }
                    }
                }
                if found.is_some() {
                    break;
                }
            }
        }

        // (4) Standalone agent_runs match
        if found.is_none() {
            for ar in state_guard.agent_runs.values() {
                let ar_agent_id = ar.agent_id.as_deref().unwrap_or(&ar.id);
                if ar_agent_id == agent_id
                    || ar.id == agent_id
                    || ar_agent_id.starts_with(&agent_id)
                    || ar.id.starts_with(&agent_id)
                {
                    found = Some(ar_agent_id.to_string());
                    break;
                }
            }
        }

        found
    };

    if let Some(aid) = resolved_agent_id {
        let event = Event::AgentInput {
            agent_id: AgentId::new(aid),
            input: message,
        };
        event_bus
            .send(event)
            .map_err(|_| ConnectionError::WalError)?;
        return Ok(Response::Ok);
    }

    // (5) Session liveness check: before returning 'not found', verify the
    // tmux session isn't still alive (recovery scenario where state is stale)
    let session_alive = tokio::process::Command::new("tmux")
        .args(["has-session", "-t", &agent_id])
        .output()
        .await
        .map(|o| o.status.success())
        .unwrap_or(false);

    if session_alive {
        let event = Event::AgentInput {
            agent_id: AgentId::new(&agent_id),
            input: message,
        };
        event_bus
            .send(event)
            .map_err(|_| ConnectionError::WalError)?;
        return Ok(Response::Ok);
    }

    Ok(Response::Error {
        message: format!("Agent not found: {}", agent_id),
    })
}

/// Handle pipeline prune requests.
///
/// Removes terminal pipelines (failed/cancelled/done) from state and
/// cleans up their log files. By default only prunes pipelines older
/// than 12 hours; use `--all` to prune all terminal pipelines.
// TODO(refactor): group prune flags into a shared struct with handle_agent_prune/handle_workspace_prune
#[allow(clippy::too_many_arguments)]
pub(super) fn handle_pipeline_prune(
    state: &Arc<Mutex<MaterializedState>>,
    event_bus: &EventBus,
    logs_path: &std::path::Path,
    orphans_registry: &Arc<Mutex<Vec<oj_engine::breadcrumb::Breadcrumb>>>,
    all: bool,
    failed: bool,
    orphans: bool,
    dry_run: bool,
    namespace: Option<&str>,
) -> Result<Response, ConnectionError> {
    let now_ms = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap_or_default()
        .as_millis() as u64;
    let age_threshold_ms = 12 * 60 * 60 * 1000; // 12 hours in ms

    let mut to_prune = Vec::new();
    let mut skipped = 0usize;

    // When --orphans is used alone, skip the normal terminal-pipeline loop.
    // When combined with --all or --failed, run both.
    let prune_terminal = all || failed || !orphans;

    if prune_terminal {
        let state_guard = state.lock();
        for pipeline in state_guard.pipelines.values() {
            // Filter by namespace when --project is specified
            if let Some(ns) = namespace {
                if pipeline.namespace != ns {
                    continue;
                }
            }

            if !pipeline.is_terminal() {
                skipped += 1;
                continue;
            }

            // --failed flag: only prune failed pipelines (skip done/cancelled)
            if failed && pipeline.step != "failed" {
                skipped += 1;
                continue;
            }

            // Determine if this pipeline skips the age check:
            // - --all: everything skips age check
            // - --failed: failed pipelines skip age check
            // - cancelled pipelines always skip age check (default behavior)
            let skip_age_check =
                all || (failed && pipeline.step == "failed") || pipeline.step == "cancelled";

            if !skip_age_check {
                let created_at_ms = pipeline
                    .step_history
                    .first()
                    .map(|r| r.started_at_ms)
                    .unwrap_or(0);
                if created_at_ms > 0 && now_ms.saturating_sub(created_at_ms) < age_threshold_ms {
                    skipped += 1;
                    continue;
                }
            }

            to_prune.push(PipelineEntry {
                id: pipeline.id.clone(),
                name: pipeline.name.clone(),
                step: pipeline.step.clone(),
            });
        }
    }

    if !dry_run {
        for entry in &to_prune {
            // Emit PipelineDeleted event to remove from state
            let event = Event::PipelineDeleted {
                id: PipelineId::new(entry.id.clone()),
            };
            event_bus
                .send(event)
                .map_err(|_| ConnectionError::WalError)?;

            // Best-effort cleanup of pipeline log and breadcrumb files
            let log_file = oj_engine::log_paths::pipeline_log_path(logs_path, &entry.id);
            let _ = std::fs::remove_file(&log_file);
            let crumb_file = oj_engine::log_paths::breadcrumb_path(logs_path, &entry.id);
            let _ = std::fs::remove_file(&crumb_file);

            // Best-effort cleanup of agent log files for this pipeline's steps
            // Agent logs are at logs_path/agent/<agent_id>.log
            // The pipeline ID is used as agent_id for agent steps
            let agent_log = logs_path.join("agent").join(format!("{}.log", entry.id));
            let _ = std::fs::remove_file(&agent_log);
            let agent_dir = logs_path.join("agent").join(&entry.id);
            let _ = std::fs::remove_dir_all(&agent_dir);
        }
    }

    // When --orphans flag is set, collect orphaned pipelines
    if orphans {
        let mut orphan_guard = orphans_registry.lock();
        let drain_indices: Vec<usize> = (0..orphan_guard.len()).collect();
        for &i in drain_indices.iter().rev() {
            let bc = &orphan_guard[i];
            to_prune.push(PipelineEntry {
                id: bc.pipeline_id.clone(),
                name: bc.name.clone(),
                step: "orphaned".to_string(),
            });
            if !dry_run {
                let removed = orphan_guard.remove(i);
                // Delete breadcrumb file
                let crumb = oj_engine::log_paths::breadcrumb_path(logs_path, &removed.pipeline_id);
                let _ = std::fs::remove_file(&crumb);
                // Delete pipeline log
                let log_file =
                    oj_engine::log_paths::pipeline_log_path(logs_path, &removed.pipeline_id);
                let _ = std::fs::remove_file(&log_file);
                // Delete agent logs/dirs
                let agent_log = logs_path
                    .join("agent")
                    .join(format!("{}.log", removed.pipeline_id));
                let _ = std::fs::remove_file(&agent_log);
                let agent_dir = logs_path.join("agent").join(&removed.pipeline_id);
                let _ = std::fs::remove_dir_all(&agent_dir);
            }
        }
    }

    Ok(Response::PipelinesPruned {
        pruned: to_prune,
        skipped,
    })
}

/// Handle agent prune requests.
///
/// Removes agent log files for agents belonging to terminal pipelines
/// (failed/cancelled/done). By default only prunes agents from pipelines
/// older than 24 hours; use `--all` to prune all.
pub(super) fn handle_agent_prune(
    state: &Arc<Mutex<MaterializedState>>,
    event_bus: &EventBus,
    logs_path: &std::path::Path,
    all: bool,
    dry_run: bool,
) -> Result<Response, ConnectionError> {
    let now_ms = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap_or_default()
        .as_millis() as u64;
    let age_threshold_ms = 24 * 60 * 60 * 1000; // 24 hours in ms

    let mut to_prune = Vec::new();
    let mut pipeline_ids_to_delete = Vec::new();
    let mut skipped = 0usize;

    {
        let state_guard = state.lock();
        for pipeline in state_guard.pipelines.values() {
            if !pipeline.is_terminal() {
                skipped += 1;
                continue;
            }

            // Check age via step history
            if !all {
                let created_at_ms = pipeline
                    .step_history
                    .first()
                    .map(|r| r.started_at_ms)
                    .unwrap_or(0);
                if created_at_ms > 0 && now_ms.saturating_sub(created_at_ms) < age_threshold_ms {
                    skipped += 1;
                    continue;
                }
            }

            // Collect agents from step history
            for record in &pipeline.step_history {
                if let Some(agent_id) = &record.agent_id {
                    to_prune.push(AgentEntry {
                        agent_id: agent_id.clone(),
                        pipeline_id: pipeline.id.clone(),
                        step_name: record.name.clone(),
                    });
                }
            }

            pipeline_ids_to_delete.push(pipeline.id.clone());
        }
    }

    if !dry_run {
        // Delete the terminal pipelines from state so agents no longer appear in `agent list`
        for pipeline_id in &pipeline_ids_to_delete {
            let event = Event::PipelineDeleted {
                id: PipelineId::new(pipeline_id.clone()),
            };
            event_bus
                .send(event)
                .map_err(|_| ConnectionError::WalError)?;

            // Best-effort cleanup of pipeline log and breadcrumb files
            let log_file = oj_engine::log_paths::pipeline_log_path(logs_path, pipeline_id);
            let _ = std::fs::remove_file(&log_file);
            let crumb_file = oj_engine::log_paths::breadcrumb_path(logs_path, pipeline_id);
            let _ = std::fs::remove_file(&crumb_file);
        }

        for entry in &to_prune {
            // Best-effort cleanup of agent log file
            let agent_log = logs_path
                .join("agent")
                .join(format!("{}.log", entry.agent_id));
            let _ = std::fs::remove_file(&agent_log);

            // Best-effort cleanup of agent log directory
            let agent_dir = logs_path.join("agent").join(&entry.agent_id);
            let _ = std::fs::remove_dir_all(&agent_dir);
        }
    }

    Ok(Response::AgentsPruned {
        pruned: to_prune,
        skipped,
    })
}

/// Handle workspace prune requests.
///
/// Iterates `$OJ_STATE_DIR/workspaces/` children on the filesystem.
/// For each directory: if it has a `.git` file (indicating a git worktree),
/// best-effort `git worktree remove`; then `rm -rf` regardless.
pub(super) async fn handle_workspace_prune(
    state: &Arc<Mutex<MaterializedState>>,
    all: bool,
    dry_run: bool,
    namespace: Option<&str>,
) -> Result<Response, ConnectionError> {
    let state_dir = std::env::var("OJ_STATE_DIR").unwrap_or_else(|_| {
        format!(
            "{}/.local/state/oj",
            std::env::var("HOME").unwrap_or_default()
        )
    });
    let workspaces_dir = std::path::PathBuf::from(&state_dir).join("workspaces");

    // When filtering by namespace, build a set of workspace IDs that match.
    // Namespace is derived from the workspace's owner (pipeline or worker).
    let namespace_filter: Option<std::collections::HashSet<String>> = namespace.map(|ns| {
        let state_guard = state.lock();
        state_guard
            .workspaces
            .iter()
            .filter(|(_, w)| {
                w.owner
                    .as_deref()
                    .and_then(|owner| {
                        state_guard
                            .pipelines
                            .get(owner)
                            .map(|p| p.namespace.as_str())
                            .or_else(|| {
                                state_guard
                                    .workers
                                    .get(owner)
                                    .map(|wr| wr.namespace.as_str())
                            })
                    })
                    .unwrap_or_default()
                    == ns
            })
            .map(|(id, _)| id.clone())
            .collect()
    });

    let mut to_prune = Vec::new();
    let mut skipped = 0usize;

    // Read immediate children of the workspaces directory
    let entries = match tokio::fs::read_dir(&workspaces_dir).await {
        Ok(entries) => entries,
        Err(_) => {
            // Directory doesn't exist or isn't readable — nothing to prune
            return Ok(Response::WorkspacesPruned {
                pruned: Vec::new(),
                skipped: 0,
            });
        }
    };

    let now = std::time::SystemTime::now();
    let age_threshold = std::time::Duration::from_secs(12 * 60 * 60);

    let mut entries = entries;
    while let Ok(Some(entry)) = entries.next_entry().await {
        let path = entry.path();
        if !path.is_dir() {
            continue;
        }

        let id = entry.file_name().to_string_lossy().to_string();

        // Filter by namespace when --project is specified
        if let Some(ref allowed_ids) = namespace_filter {
            if !allowed_ids.contains(&id) {
                continue;
            }
        }

        // Check age via directory mtime (skip if < 12h unless --all)
        if !all {
            if let Ok(metadata) = tokio::fs::metadata(&path).await {
                if let Ok(modified) = metadata.modified() {
                    if let Ok(age) = now.duration_since(modified) {
                        if age < age_threshold {
                            skipped += 1;
                            continue;
                        }
                    }
                }
            }
        }

        to_prune.push(WorkspaceEntry {
            id,
            path,
            branch: None,
        });
    }

    if !dry_run {
        for ws in &to_prune {
            // If the directory has a .git file (not directory), it's a git worktree
            let dot_git = ws.path.join(".git");
            if tokio::fs::symlink_metadata(&dot_git)
                .await
                .map(|m| m.is_file())
                .unwrap_or(false)
            {
                // Best-effort git worktree remove (ignore failures).
                // Run from within the worktree so git can locate the parent repo.
                let _ = tokio::process::Command::new("git")
                    .arg("worktree")
                    .arg("remove")
                    .arg("--force")
                    .arg(&ws.path)
                    .current_dir(&ws.path)
                    .output()
                    .await;
            }

            // Remove directory regardless
            let _ = tokio::fs::remove_dir_all(&ws.path).await;
        }
    }

    Ok(Response::WorkspacesPruned {
        pruned: to_prune,
        skipped,
    })
}

/// Handle worker prune requests.
///
/// Removes all stopped workers from state by emitting WorkerDeleted events.
/// Workers are either "running" or "stopped" — all stopped workers are eligible
/// for pruning with no age threshold.
pub(super) fn handle_worker_prune(
    state: &Arc<Mutex<MaterializedState>>,
    event_bus: &EventBus,
    _all: bool,
    dry_run: bool,
    namespace: Option<&str>,
) -> Result<Response, ConnectionError> {
    let mut to_prune = Vec::new();
    let mut skipped = 0usize;

    {
        let state_guard = state.lock();
        for record in state_guard.workers.values() {
            // Filter by namespace if specified
            if let Some(ns) = namespace {
                if record.namespace != ns {
                    continue;
                }
            }
            if record.status != "stopped" {
                skipped += 1;
                continue;
            }
            to_prune.push(WorkerEntry {
                name: record.name.clone(),
                namespace: record.namespace.clone(),
            });
        }
    }

    if !dry_run {
        for entry in &to_prune {
            let event = Event::WorkerDeleted {
                worker_name: entry.name.clone(),
                namespace: entry.namespace.clone(),
            };
            event_bus
                .send(event)
                .map_err(|_| ConnectionError::WalError)?;
        }
    }

    Ok(Response::WorkersPruned {
        pruned: to_prune,
        skipped,
    })
}

/// Handle cron prune requests.
///
/// Removes all stopped crons from state by emitting CronDeleted events.
/// Crons are either "running" or "stopped" — all stopped crons are eligible
/// for pruning with no age threshold.
pub(super) fn handle_cron_prune(
    state: &Arc<Mutex<MaterializedState>>,
    event_bus: &EventBus,
    _all: bool,
    dry_run: bool,
) -> Result<Response, ConnectionError> {
    let mut to_prune = Vec::new();
    let mut skipped = 0usize;

    {
        let state_guard = state.lock();
        for record in state_guard.crons.values() {
            if record.status != "stopped" {
                skipped += 1;
                continue;
            }
            to_prune.push(CronEntry {
                name: record.name.clone(),
                namespace: record.namespace.clone(),
            });
        }
    }

    if !dry_run {
        for entry in &to_prune {
            let event = Event::CronDeleted {
                cron_name: entry.name.clone(),
                namespace: entry.namespace.clone(),
            };
            event_bus
                .send(event)
                .map_err(|_| ConnectionError::WalError)?;
        }
    }

    Ok(Response::CronsPruned {
        pruned: to_prune,
        skipped,
    })
}

/// Handle an agent resume request.
///
/// Finds the agent by ID/prefix (or all dead agents when `all` is true),
/// optionally kills the tmux session, then emits PipelineResume to trigger
/// the engine's resume flow (which uses `--resume` to preserve conversation).
pub(super) async fn handle_agent_resume(
    state: &Arc<Mutex<MaterializedState>>,
    event_bus: &EventBus,
    agent_id: String,
    kill: bool,
    all: bool,
) -> Result<Response, ConnectionError> {
    // Collect (pipeline_id, agent_id, session_id) tuples to resume
    // Use a scoped block to ensure lock is released before any await points
    let (targets, skipped) = {
        let state_guard = state.lock();
        let mut targets: Vec<(String, String, Option<String>)> = Vec::new();
        let mut skipped: Vec<(String, String)> = Vec::new();

        if all {
            // Iterate all non-terminal pipelines, find ones with agents
            for pipeline in state_guard.pipelines.values() {
                if pipeline.is_terminal() {
                    continue;
                }
                // Get the current step's agent
                if let Some(record) = pipeline
                    .step_history
                    .iter()
                    .rfind(|r| r.name == pipeline.step)
                {
                    if let Some(ref aid) = record.agent_id {
                        if !kill {
                            // Without --kill, only resume agents that are
                            // escalated/waiting (dead session scenario)
                            if !pipeline.step_status.is_waiting()
                                && !matches!(
                                    pipeline.step_status,
                                    oj_core::StepStatus::Failed | oj_core::StepStatus::Pending
                                )
                            {
                                skipped.push((
                                    aid.clone(),
                                    format!(
                                        "agent is {:?} (use --kill to force)",
                                        pipeline.step_status
                                    ),
                                ));
                                continue;
                            }
                        }
                        targets.push((
                            pipeline.id.clone(),
                            aid.clone(),
                            pipeline.session_id.clone(),
                        ));
                    }
                }
            }
        } else {
            // Find specific agent by ID or prefix
            let mut found = false;
            for pipeline in state_guard.pipelines.values() {
                for record in &pipeline.step_history {
                    if let Some(ref aid) = record.agent_id {
                        if aid == &agent_id || aid.starts_with(&agent_id) {
                            if pipeline.is_terminal() {
                                return Ok(Response::Error {
                                    message: format!(
                                        "pipeline {} is already {} — cannot resume agent",
                                        pipeline.id, pipeline.step
                                    ),
                                });
                            }
                            targets.push((
                                pipeline.id.clone(),
                                aid.clone(),
                                pipeline.session_id.clone(),
                            ));
                            found = true;
                            break;
                        }
                    }
                }
                if found {
                    break;
                }
            }

            if !found {
                return Ok(Response::Error {
                    message: format!("agent not found: {}", agent_id),
                });
            }
        }

        (targets, skipped)
    };

    // If --kill is specified, kill the tmux sessions first
    if kill {
        for (_, _, session_id) in &targets {
            if let Some(sid) = session_id {
                // Kill the tmux session (ignore errors - session may already be dead)
                let _ = tokio::process::Command::new("tmux")
                    .args(["kill-session", "-t", sid])
                    .output()
                    .await;

                // Emit SessionDeleted to clean up state
                let event = Event::SessionDeleted {
                    id: SessionId::new(sid),
                };
                let _ = event_bus.send(event);
            }
        }
    }

    let mut resumed = Vec::new();

    for (pipeline_id, aid, _) in targets {
        let event = Event::PipelineResume {
            id: PipelineId::new(&pipeline_id),
            message: None,
            vars: std::collections::HashMap::new(),
        };
        event_bus
            .send(event)
            .map_err(|_| ConnectionError::WalError)?;
        resumed.push(aid);
    }

    Ok(Response::AgentResumed { resumed, skipped })
}

/// Validate that agent steps have a message for resume.
///
/// Returns `Ok(())` if validation passes, or `Err(message)` with an error message
/// if the step is an agent step and no message was provided.
fn validate_resume_message(
    state: &Arc<Mutex<MaterializedState>>,
    pipeline_id: &str,
    pipeline_kind: &str,
    current_step: &str,
    runbook_hash: &str,
) -> Result<(), String> {
    // Get the stored runbook
    let stored = {
        let state_guard = state.lock();
        state_guard.runbooks.get(runbook_hash).cloned()
    };

    let Some(stored) = stored else {
        // If runbook is not found, let the engine handle it
        return Ok(());
    };

    // Parse the runbook
    let runbook: Runbook = match serde_json::from_value(stored.data) {
        Ok(rb) => rb,
        Err(_) => {
            // If we can't parse, let the engine handle it
            return Ok(());
        }
    };

    // Get the pipeline and step definitions
    let Some(pipeline_def) = runbook.get_pipeline(pipeline_kind) else {
        return Ok(());
    };
    let Some(step_def) = pipeline_def.get_step(current_step) else {
        return Ok(());
    };

    // Check if it's an agent step
    if step_def.is_agent() {
        let short_id = pipeline_id.short(12);
        return Err(format!(
            "agent steps require --message for resume. Example:\n  \
             oj pipeline resume {} -m \"I fixed the import, try again\"",
            short_id
        ));
    }

    Ok(())
}

#[cfg(test)]
#[path = "mutations_tests.rs"]
mod tests;
